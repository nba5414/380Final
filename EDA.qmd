---
title: "Final Report"
format: pdf
editor: visual
authors: Nicholas Allen, Surya Maddali, Jake Adams
---

# Introduction:

In recent decades, cell phones have become a hot commodity around the world. The idea of calling with the tips of your fingers was a revolutionary idea that continues to set the standard for telecommunications. With advancements to cell phones, one question that is always present is pricing There may be certain factors that affect cell phone pricing such as storage, camera capabilities, and battery power. The goal of this project is to assess that, seeing if certain features of phones affect pricing in a significant way. It is an interesting and important question to answer because it can inform others about what phone features matter the most to companies that make phones as well as inform us about what features matter the most to a phone's functionality when looking to buy one. Machine learning is a reasonable approach to tackle this question because it can give us insight into why or how phones are purchased. Moreover, it can help us predict phone prices in the future based on what features they possess, which would be informed by past data on this exact matter. In other words, it would help readers assess what features are continuing to affect the price of the phone the most in the present.

# Illustration:

![Elephant](380_Final_Chart_PNG.png)

# Background and Related Works:

We looked at an article from IEEE Xplore. This article was about predicting mobile phone prices using a data set from kaggle. This article differed from ours because they were predicting phone prices with classification. They had their y variable in as a factor with 4 levels. The levels were form "low cost" to "very high cost". Some examples of their x variables were battery power and clock speed. They used several different models to predict phone price such as a decision tree and SVM. Their most accurate model was SVM with an accuracy of 94.8%.

Reference: N. Hu, "Classification of Mobile Phone Price Dataset Using Machine Learning Algorithms," 2022 3rd International Conference on Pattern Recognition and Machine Learning (PRML), Chengdu, China, 2022, pp. 438-443, doi: 10.1109/PRML56267.2022.9882236. keywords: {Support vector machines;Machine learning algorithms;Random access memory;Machine learning;Feature extraction;Mobile handsets;Batteries;computer science;machine learning;classification;price prediction},

# Data Processing:

We loaded in the data sets though the readxl package

```{r}
#| echo: false

library(glmnet)
library(readxl)
library(tidyverse)
library(corrplot)
library(torch)
library(luz)
library(dplyr)
library(broom)
library(purrr)
library(caret)
library(tibble)



df <- read_excel('smartphones_-_smartphones.xlsx')
df2 <- read_csv('Sales.csv')
```

## First Dataset

The first data set looked like this before processing.

```{r}
#| echo: false
head(df)
```

It is a tabular data set on some mobile phones. Some examples of columns in the data set are mobile which represents the name of the phone and the price of the phone.

To start off we took out the model column because it represented the names of the phones which will not impact the price. We also took out the sim column.

```{r}
#| echo: false
df <- df %>% drop_na() %>% select(!model) %>% select(!sim)
head(df)
```

### Cleaning battery column

We extracted the battery life of each phone in mAH and made the column numeric

```{r}
#| echo: false
df <- df %>% 
  mutate(battery = gsub(pattern = "mAh Battery|with|(?:[0-9]){1,3}W|Fast Charging", replacement = "", battery)) %>% mutate_at('battery', as.numeric) %>% drop_na() %>% rename('battery_mAh'='battery')
head(df)
```

### Cleaning processor variable

We extracted the power of the processor in GHz. We then made the column numeric

```{r}
#| echo: false
df$processor <- str_extract(df$processor, "\\d+\\.?\\d*\\s*GHz|\\d+\\s*GHz")

df$processor <- gsub("GHz", "", df$processor)

df <- df %>% drop_na() %>% mutate_at('processor', as.numeric) %>%rename('processor GHz)'='processor')
head(df)
```

### Cleaning os column

We noticed that because the data was unclean, some of the values that should be in the os column were in the card column. We put these value in the os column and removed the card column after. We also made the os column a factor.

```{r}
#| echo: false
for (i in 1:nrow(df)){
  if (df[i,9] == 'No FM Radio'){
    df[i,9] <- df[i,8]
  }
  else if (df[i,9] == 'Bluetooth'){
    df[i,9] <- df[i,8]
  }
}

df <- df %>% select(!card) %>% mutate_at('os', as.factor)
head(df)
```

### Cleaning camera column

We extracted the amount of mega pixels in the front camera of each phone. We made this column numeric

```{r}
#| echo: false
df$camera <- str_extract(df$camera, '[0-9]{1,2} MP Front Camera')
df$camera <- str_extract(df$camera, '[0-9]{1,2}')
df <- df %>% mutate_at('camera', as.numeric) %>% rename('f_camera_MP'='camera') %>% drop_na()
head(df)
```

### Cleaning ram column

We extracted the ram of the phones in GB and made it a factor because phones only have a few preset values for their ram

```{r}
#| echo: false
df$ram <- str_extract(df$ram, '[0-9]{1,2} GB')
df <- df %>% mutate_at('ram', as.factor) 
head(df)
```

### Cleaning Display column

We extracted the display size and the Hz of the display and turned that into two new columns. We made these new columns numeric and removed the original

```{r}
#| echo: false
df <- df %>% mutate(displaySize = as.numeric(str_extract(df$display, "\\b\\d+\\.\\d+\\b")))
```

```{r}
#| echo: false
df <- df %>% mutate(displayHz = as.numeric(str_extract(df$display, "\\b\\d+(?=\\s*Hz)")))

df <- df %>% select(!display)
head(df)
```

### Cleaning Price column

We converted the value in rupees to dollars to make it easier to understand for our audience

```{r}
#| echo: false
df <- df %>%
  mutate(price = gsub(",", "", price))
df$price <- sub("\\₹", "", df$price)
df$price <- as.numeric(df$price)

df <- df %>%
  mutate(price = round(price / 83.41, digits = 2)) %>% rename('price'='price') %>% drop_na()
head(df)
```

### General Analysis

After Cleaning:

```{r}
#| echo: false
head(df)
```

```{r}
#| echo: false
summary(df)
```

```{r}
#| echo: false
library(corrplot)

numeric_data <- df2 %>%
  select_if(is.numeric) %>% drop_na()

correlation_matrix <- cor(numeric_data)

corrplot(correlation_matrix, method = "circle")
```

## Second Dataset

The first dataset looked like this before processing

```{r}
#| echo: false
head(df2)
```

It is also a tabular data set with information on mobile phones. This data set differs from the first because it has less columns that are useful for predicting price but it has more rows.

### Cleaning P1

To start we removed unneeded columns. These were models, Camera, selling price, mobile, discount, and discount percentage. We then make all the column names lowercase. We then made all the data in the colors and brands columns lowercase. We then removed the underscore from the original_price column name. We then converted the price to dollars. We them made the memory, brands, and storage columns factors.

```{r}
#| echo: false
df2 <- df2 %>% drop_na()

df2 <- df2[,-c(2,3,6,8,10,11,12)]

names(df2) <- tolower(names(df2))

df2$brands <- tolower(df2$brands)

df2<- rename(df2, original_price = "original price")

df2 <- df2 %>% mutate(original_price = df2$original_price * 0.012)

df2$memory <- as.factor(df2$memory)

df2$storage <- as.factor(df2$storage)

df2$brands <- as.factor(df2$brands)
head(df2)
```

### General Analysis

After cleaning:

```{r}
#| echo: false
head(df2)
```

```{r}
#| echo: false
summary(df2)
```

```{r}
#| echo: false
numeric_data2 <- df %>%
  select_if(is.numeric)
correlation_matrix2 <- cor(numeric_data2)
corrplot(correlation_matrix2, method = "circle")
```

```{r}
#| echo: false
rmse <- function(y, yhat) {
sqrt(mean((y - yhat)^2))
}
```

### Model Creation: Predicting Price

To start with our introductory model, we have decided on a multi-linear regression model. This will set a foundation for the model complex models all predicting price. In the future, we plan to extend apon this will stepwise regression, and a neural network.

# Split Test and Train Data:

======= \### Model Creation: Predicting Price

To start with our introductory model, we have decided on a multi-linear regression model. This will set a foundation for the more complex models all predicting price. In the future, we plan to extend apon this will stepwise regression and a neural network.

Fit Two Linear Regression Models:

```{r}
#| echo: false
lm1 <- lm(price ~ ., data = df)
lm2 <- lm(original_price ~ ., data = df2)

summary1 <- summary(lm1)
summary2 <- summary(lm2)

```

Summary of Models:

```{r}
#| echo: false
print(summary1)
print(summary2)
```

Initial Thoughts:

We would expect to see price increase as the newer specs for phones are released and are put onto the market. Thus, we would expect things like memory, storage, ram, and other specs to be significant predictors for both of our data sets. As we can see this is the true.

One noteworthy observation we noticed in both models in the high level of Adjusted R-Squared(.85 and .75). This can indicate over fitting, or it can indicator an accurate model. This will need to be verified on the test data with some prediction tests after our interpretations.

Interpreting our coefficients:

For the first model,

Intercept: When all other predictor variables are zero, the estimated price of the product is approximately -\$5.080e+02.

Rating: For every one-unit increase in the rating, the price is estimated to increase by approximately \$1.754e+01, holding all other variables constant.

Processor(GHz): For every one-unit increase in the processor GHz, the price is estimated to increase by approximately \$1.423e+02, holding all other variables constant.

RAM(16GB): Phones with 16GB of RAM are estimated to have a price increase of approximately \$4.169e+01 compared to the reference category, holding all other variables constant.

RAM(18GB): Phones with 18GB of RAM are estimated to have a price increase of approximately \$3.340e+02 compared to the reference category, holding all other variables constant.

RAM(3GB): Phones with 3GB of RAM are estimated to have a price increase of approximately \$7.775e+01 compared to the reference category, holding all other variables constant.

RAM(4GB): Phones with 4GB of RAM are estimated to have a price decrease of approximately \$5.905e+01 compared to the reference category, holding all other variables constant.

RAM(6GB): Phones with 6GB of RAM are estimated to have a price decrease of approximately \$1.195e+02 compared to the reference category, holding all other variables constant.

Ram(8GB): Phones with 6GB of RAM are estimated to have a price decrease of approximately \$9.567e+01 compared to the reference category, holding all other variables constant.

Battery(mAH): For every one-unit increase in battery mAh, the price is estimated to decrease by approximately \$4.114e-04, holding all other variables constant.

Camera(MP): For every one-unit increase in front camera megapixels, the price is estimated to decrease by approximately \$2.566, holding all other variables constant.

Operating System: Devices with Android v10.0: On average, devices with Android v10.0 have prices that are \$22.38 higher than the reference category, holding all other variables constant. Android v11: On average, devices with Android v11 have prices that are \$82.11 lower than the reference category, holding all other variables constant. Android v12: On average, devices with Android v12 have prices that are \$69.65 lower than the reference category, holding all other variables constant. Android v13: On average, devices with Android v13 have prices that are \$58.58 lower than the reference category, holding all other variables constant. EMUI v12: On average, devices with EMUI v12 have prices that are \$80.82 lower than the reference category, holding all other variables constant. Harmony v2.0: On average, devices with Harmony v2.0 have prices that are \$140.50 lower than the reference category, holding all other variables constant. HarmonyOS: On average, devices with HarmonyOS have prices that are \$87.34 higher than the reference category, holding all other variables constant. HarmonyOS v2.0: On average, devices with HarmonyOS v2.0 have prices that are \$15.82 lower than the reference category, holding all other variables constant. Hongmeng OS v3.0: On average, devices with Hongmeng OS v3.0 have prices that are \$2303 higher than the reference category, holding all other variables constant. Hongmeng OS v4.0: On average, devices with Hongmeng OS v4.0 have prices that are \$662.20 higher than the reference category, holding all other variables constant. iOS v15 and iOS v15.0: On average, devices with iOS v15 have prices that are \$1266 higher than the reference category, and devices with iOS v15.0 have prices that are \$998.20 higher than the reference category, holding all other variables constant.

Display size: For every one-unit increase in display size, the price is estimated to decrease by approximately \$1.389e+02, holding all other variables constant.

Display Hz: For every one-unit increase in display size, the price is estimated to decrease by approximately \$1.487, holding all other variables constant.

For the second model,

Operating System: Phones of the brands ASUS, Gionee, Google Pixel, HTC, Infinix, IQOO, Lenovo, LG, Motorola, Nokia, OPPO, POCO, Realme, Samsung, Vivo, and Xiaomi have an estimated price change of -491.83, -517.99, -2.47, -281.1, -582.7, -604.93, -504.80, -398.47, -468.75, -482.1, -552.06, -606.83, -612.25, -426.85, -566.37, and -563.31 respectively in dollars compared to the reference category holding all other variables constant.

Memory: Phones with RAM memory capacity of 1.5GB, 10MB, 100 MB, 12GB, 128MB, 153MB, 16GB, 16 MB, 2GB, 2MB, 3GB, 30MB, 32MB, 4GB, 46MB, 4G, 512MB, 6GB, 64MB, 768MB, 8GB, and 8MB have an estimated price change of -7.67,-15.23,-61.86, 469.00, -18.25, -1309.63, 731.42, -9.39, -4.51, -1.56, 64.78, 49.61, 3.63, 143.76, 5.76, -11.06, 259.28, -11.84, 203.96, 8.72, -22.95, 271.17, and 13.27 respectively in dollars compared to the reference category holding all other variables constant.

Storage: Phones with storage capacity of 10MB, 100MB, 128GB, 128MB, 129GB, 130GB, 140MB, 153MB, 16GB, 16MB, 2MB, 256GB, 256MB, 32GB, 4GB, 4Mb, 512GB, 512MB, 64GB, 64MB, 8 GB, and 8 MB have an estimated price change of -1211.90, -1301.45, 1133.10,-1238.42, -1180.07, -1168.48, -1172.08, NULL(due to multicollinearity), 1160.44, -1227.40, -1299.06, 892.08, -1144.20, 1158.03, 1196.87, -1260.41, 392.65, -1224.56, 1176.79, -1225.13, 1186.76, and -1265.87 respectively in dollars when compared to the reference category holding all other predictors constant. Rating: For every one unit increase in rating we see a price change of \$64.97 when holding all other predictors constant.

Rating: For every one-unit increase in rating, the price is estimated to increase by approximately \$119.62, holding all other variables constant.

=======

```{r}
#| echo: false
linear1rmse <- rmse(df$price, lm1$fitted.values)
print(linear1rmse)
linear2rmse <- rmse(df2$original_price, lm2$fitted.values)
print(linear2rmse)

print(mean(df$price))
print(mean(df2$original_price))

```

As you see, we have a RMSE of 98.6 and 165.5 for both data sets respectively. On average, this means our model is off by about \$98.6 and \$165.5. Since the average price of the phones is around \$345.50 and \$319.88, our model is not too inaccurate, but it can be improved.

The RMSE variation is likely caused by the heavy right-skewness in the price variable of more expensive phones. More expensive prices leads to an exponential decay of more expensive components in a thinner market; thus, leading to an exponentially distributed price vector column.

```{r}
#| echo: false
histogram(df$price, main = "Boxplot of Price Variable(Df1)")

histogram(df2$original_price, main = "Boxplot of Continuous Variable(DF2)")

```

Lasso and Ridge:

```{r}
#| echo: false
library(glmnet)
x1 <- data.matrix(df %>% select(!price))
y1 <- df$price
x2 <- data.matrix(df2 %>% select(!original_price))
y2 <- df2$original_price

lasso1 <- cv.glmnet(x1, y1, alpha = 1)
ridge1 <- cv.glmnet(x1, y1, alpha = 0)
lasso2 <- cv.glmnet(x2, y2, alpha = 1)
ridge2 <- cv.glmnet(x2, y2, alpha = 0)

lasso1rmse <- rmse(y1, predict(lasso1, x1))
ridge1rmse <- rmse(y1, predict(ridge1, x1))
lasso2rmse <- rmse(y2, predict(lasso2, x2))
ridge2rmse <- rmse(y2, predict(ridge2, x2))

```

```{r}
#| echo: false
par(mfrow=c(1, 2))
plot(lasso1)
plot(ridge1)
plot(lasso2)
plot(ridge2)
```

Stepwise:

```{r}
#| echo: false
null_model1 <- lm(price ~ 1, df)
forward1 <- step(null_model1, direction = 'forward', scope = formula(lm1))

null_model2 <- lm(original_price ~ 1, df2)
forward2 <- step(null_model2, direction = 'forward', scope = formula(lm2))


backward1 <- step(lm1, direction = 'backward')
backward2 <- step(lm2, direction = 'backward')


forward1rmse <- rmse(df$price, predict(forward1))
backward1rmse <- rmse(df$price, predict(backward1))
forward2rmse <- rmse(df2$original_price, predict(forward2))
backward2rmse <- rmse(df2$original_price, predict(backward2))
```

Neural Network:


```{r}
normalize <- function(x) {
  (x - min(x)) / (max(x) - min(x))
}

df_normalized <- df %>%
  mutate(price = normalize(price)) 

df2_normalized <- df2 %>%
  mutate(original_price = normalize(original_price))
  

```


```{r}
#| echo: false
nn_model <- nn_module(
  initialize = function(p, q1, q2, q3){
    self$hidden1 <- nn_linear(p,q1)
    self$hidden2 <- nn_linear(q1,q2)
    self$hidden3 <- nn_linear(q2,q3)
    self$output <- nn_linear(q3,1)
    self$activation <- nn_relu()
  },
  forward = function(x){
    x %>%
      self$hidden1() %>% self$activation() %>%
      self$hidden2() %>% self$activation() %>%
      self$hidden3() %>% self$activation() %>%
      self$output()
  }
)

```

```{r}
#| echo: false
M1 <- model.matrix(price ~ 0 + . , data = df_normalized)

nn1 <- nn_model %>% 
    setup(loss = nn_mse_loss(),
        optimizer = optim_adam, 
        metrics = list(luz_metric_accuracy())) %>%
    set_hparams(p = ncol(M1), q1 = 16, q2 = 32, q3 = 16) %>%
    set_opt_hparams(lr = 0.05) %>%
    fit(data = list(
        model.matrix(price ~ 0 + ., data = df_normalized), df_normalized %>% select(price) %>% as.matrix
    ),
     epochs = 50, verbose = TRUE)

```
```{r}

unnormalize <- function(x){
  x <- x * (max(df2$original_price) - min(df2$original_price)) +  min(df2$original_price)
  return(x)
}
  

```

```{r}
#| echo: false
nnrmse1 <- rmse(df_normalized$price, predict(nn1, model.matrix(price ~ 0 + ., data = df_normalized))) %>% as.double()
```

```{r}
#| echo: false
M2 <- model.matrix(normalize(original_price) ~ 0 + . , data = df2)

nn2 <- nn_model %>% 
    setup(loss = nn_mse_loss(),
        optimizer = optim_adam, 
        metrics = list(luz_metric_accuracy())) %>%
    set_hparams(p = ncol(M2), q1 = 16, q2 = 32, q3 = 16) %>%
    set_opt_hparams(lr = 0.05) %>%
    fit(data = list(
        model.matrix(original_price ~ 0 + ., data = df2_normalized), df2_normalized %>% select(original_price) %>% as.matrix
    ),
     epochs = 50, verbose = TRUE)
```

```{r}
#| echo: false


nnrmse2 <- rmse(df2_normalized$original_price, predict(nn2, model.matrix(original_price ~ 0 + ., data = df2_normalized))) %>% as.double()


```

Interpretting Normalized RMSE: 

```{r}
price_diff1<- nnrmse1 * mean(df$price)
price_diff2<- nnrmse2 * mean(df2$original_price)
print(price_diff1)
print(price_diff2)


```


Normalizing our price column leads to a different interpretation of our neural network's RMSE. On average, our first data set's model is off by about 10%. This means price can be off between +- $34.66. The best performing model by far. For our second data set, the normalized rmse is .221. This means on average, the second data set's model is off by 22.1%. Thus, price can be off by between +- $68.01. Another good model in relation to the others!



Summary of results

```{r}
#| echo: false
summary_table <- data.frame(Model = c('Linear', 'Lasso', 'Ridge', 'Forwards', 'Backwards', 'NNetwork(normalized)', 'Linear', 'Lasso', 'Ridge', 'Forwards', 'Backwards', 'NNetwork(normalized)'), Dataset = c(1,1,1,1,1,1,2,2,2,2,2,2), RMSE = c(linear1rmse, lasso1rmse, ridge1rmse, forward1rmse, backward1rmse, nnrmse1, linear2rmse, lasso2rmse, ridge2rmse, forward2rmse, backward2rmse, nnrmse2))

summary_table
```
