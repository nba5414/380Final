---
title: "EDA"
format: pdf
editor: visual
authors: Nicholas Allen, Surya Maddali, Jake Adams
---

# Introduction:

In recent decades, cell phones have become a hot commodity around the world. The idea of calling with the tips of your fingers was a revolutionary idea that continues to set the standard for telecommunications. With advancements to cell phones, one question that is always present is pricing There may be certain factors that affect cell phone pricing such as storage, camera capabilities, and battery power. The goal of this project is to assess that, seeing if certain features of phones affect pricing in a significant way. It is an interesting and important question to answer because it can inform others about what phone features matter the most to companies that make phones as well as inform us about what features matter the most to a phone's functionality when looking to buy one. Machine learning is a reasonable approach to tackle this question because it can give us insight into why or how phones are purchased. Moreover, it can help us predict phone prices in the future based on what features they possess, which would be informed by past data on this exact matter. In other words, it would help readers assess what features are continuing to affect the price of the phone the most in the present.

# Illustration:

![Elephant](380_Final_Chart_PNG.png)

# Background and Related Works:

We looked at an article from IEEE Xplore. This article was about predicting mobile phone prices using a data set from kaggle. This article differed from ours because they were predicting phone prices with classification. They had their y variable in as a factor with 4 levels. The levels were form "low cost" to "very high cost". Some examples of their x variables were battery power and clock speed. They used several different models to predict phone price such as a decision tree and SVM. Their most accurate model was SVM with an accuracy of 94.8%.

Reference: N. Hu, "Classification of Mobile Phone Price Dataset Using Machine Learning Algorithms," 2022 3rd International Conference on Pattern Recognition and Machine Learning (PRML), Chengdu, China, 2022, pp. 438-443, doi: 10.1109/PRML56267.2022.9882236. keywords: {Support vector machines;Machine learning algorithms;Random access memory;Machine learning;Feature extraction;Mobile handsets;Batteries;computer science;machine learning;classification;price prediction},

# Data Processing:

We loaded in the data sets though the readxl package

```{r}
library(readxl)
library(tidyverse)

df <- read_excel('smartphones_-_smartphones.xlsx')
df2 <- read_csv('Sales.csv')
```

## First Dataset

The first data set looked like this before processing.

```{r}
head(df)
```

It is a tabular data set on some mobile phones. Some examples of columns in the data set are mobile which represents the name of the phone and the price of the phone.

To start off we took out the model column because it represented the names of the phones which will not impact the price. We also took out the sim column.

```{r}
df <- df %>% drop_na() %>% select(!model) %>% select(!sim)
```

### Cleaning battery column

We extracted the battery life of each phone in mAH and made the column numeric

```{r}
df <- df %>% 
  mutate(battery = gsub(pattern = "mAh Battery|with|(?:[0-9]){1,3}W|Fast Charging", replacement = "", battery)) %>% mutate_at('battery', as.numeric) %>% drop_na() %>% rename('battery mAh'='battery')
```

### Cleaning processor variable

We extracted the power of the processor in GHz. We then made the column numeric

```{r}
df$processor <- str_extract(df$processor, "\\d+\\.?\\d*\\s*GHz|\\d+\\s*GHz")

df$processor <- gsub("GHz", "", df$processor)

df <- df %>% drop_na() %>% mutate_at('processor', as.numeric) %>%rename('processor GHz)'='processor')
```

### Cleaning os column

We noticed that because the data was unclean, some of the values that should be in the os column were in the card column. We put these value in the os column and removed the card column after. We also made the os column a factor.

```{r}
for (i in 1:nrow(df)){
  if (df[i,9] == 'No FM Radio'){
    df[i,9] <- df[i,8]
  }
  else if (df[i,9] == 'Bluetooth'){
    df[i,9] <- df[i,8]
  }
}

df <- df %>% select(!card) %>% mutate_at('os', as.factor)  
```

### Cleaning camera column

We extracted the amount of mega pixels in the front camera of each phone. We made this column numeric

```{r}
df$camera <- str_extract(df$camera, '[0-9]{1,2} MP Front Camera')
df$camera <- str_extract(df$camera, '[0-9]{1,2}')
df <- df %>% mutate_at('camera', as.numeric) %>% rename('f camera MP'='camera') %>% drop_na()
```

### Cleaning ram column

We extracted the ram of the phones in GB and made it a factor because phones only have a few preset values for their ram

```{r}
df$ram <- str_extract(df$ram, '[0-9]{1,2} GB')
df <- df %>% mutate_at('ram', as.factor) 
```

### Cleaning Display column

We extracted the display size and the Hz of the display and turned that into two new columns. We made these new columns numeric and removed the original

```{r}
df <- df %>% mutate(displaySize = as.numeric(str_extract(df$display, "\\b\\d+\\.\\d+\\b")))
```

```{r}
df <- df %>% mutate(displayHz = as.numeric(str_extract(df$display, "\\b\\d+(?=\\s*Hz)")))

df <- df %>% select(!display)
```

### Cleaning Price column

We converted the value in rupees to dollars to make it easier to understand for our audience

```{r}
df <- df %>%
  mutate(price = gsub(",", "", price))
df$price <- sub("\\₹", "", df$price)
df$price <- as.numeric(df$price)

df <- df %>%
  mutate(price = round(price / 83.41, digits = 2)) %>% rename('price'='price') %>% drop_na()
```

### General Analysis

After Cleaning:

```{r}
head(df)
```

```{r}
summary(df)
```

```{r}
library(corrplot)

numeric_data <- df2 %>%
  select_if(is.numeric) %>% drop_na()

correlation_matrix <- cor(numeric_data)

corrplot(correlation_matrix, method = "circle")
```

## Second Dataset

The first dataset looked like this before processing

```{r}
head(df2)
```

It is also a tabular data set with information on mobile phones. This data set differs from the first because it has less columns that are useful for predicting price but it has more rows.

### Cleaning P1

To start we removed unneeded columns. These were models, Camera, selling price, mobile, discount, and discount percentage. We then make all the column names lowercase. We then made all the data in the colors and brands columns lowercase. We then removed the underscore from the original_price column name. We then converted the price to dollars. We them made the memory, brands, and storage columns factors.

```{r}
df2 <- df2 %>% drop_na()

df2 <- df2[,-c(2,3,6,8,10,11,12)]

names(df2) <- tolower(names(df2))

df2$brands <- tolower(df2$brands)

df2<- rename(df2, original_price = "original price")

df2 <- df2 %>% mutate(original_price = df2$original_price * 0.012)

df2$memory <- as.factor(df2$memory)

df2$storage <- as.factor(df2$storage)

df2$brands <- as.factor(df2$brands)

```

### General Analysis

After cleaning:

```{r}
head(df2)
```

```{r}
summary(df2)
```

```{r}
numeric_data2 <- df %>%
  select_if(is.numeric)
correlation_matrix2 <- cor(numeric_data2)
corrplot(correlation_matrix2, method = "circle")
```

# Neural Network
```{r}
library(e1071)
library(luz)
library(torch)
library(caret)




```
## Split

```{r}
set.seed(40)
test_ind <- sample(1:nrow(df), floor(nrow(df)/5), replace = FALSE)
test_ind




```

```{r}

df_train <- df[-test_ind, ]
df_test <- df[test_ind, ]

nrow(df_train) + nrow(df_test)


```
## Benchmark
```{r}
fit_lm <- lm(price ~ .,data = df_train)
lm_test <- predict(fit_lm, df_test, output = "reponse")

lm_preds <- ifelse(lm_test > 0.5, 1, 0)
table(lm_preds, df_test$price)

```
## Neural Network Model

```{r}

nn_model <- nn_module(
  initialize = function(p, q1, q2, q3){
    self$hidden1 <- nn_linear(p,q1)
    self$hidden2 <- nn_linear(q1, q2)
    self$hidden3 <- nn_linear(q2,q3)
    self$output <- nn_linear(q3,1)
    self$activation <- nn_relu()
  },
  forward = function(x){
    x %>%
      self$hidden1() %>% self$activation() %>%
      self$hidden2() %>% self$activation() %>%
      self$hidden3() %>% self$activation() %>%
      self$output()
  }
)




```

## Fit

```{r}

M <- model.matrix(price ~ 0 + . , data = df_train)

```

```{r}
fit_nn <- nn_model %>% 
    setup(loss = nn_mse_loss(),
        optimizer = optim_adam, 
        metrics = list(luz_metric_accuracy())) %>%
    set_hparams(p = ncol(M), q1 = 128, q2 = 64, q3 = 36) %>%
    set_opt_hparams(lr = 0.005) %>%
    fit(data = list(
        model.matrix(price ~ 0 + ., data = df_train), df_train %>% select(price) %>% as.matrix
    ),
        valid_data = list(
        model.matrix(price ~ 0 + ., data = df_test), df_test %>% select(price) %>% as.matrix

        ),
        epochs = 10, verbose = TRUE)
```

