---
title: "Final Report"
format: pdf
editor: visual
authors: Nicholas Allen, Surya Maddali, Jake Adams
---

# Introduction:

In recent decades, cell phones have become a hot commodity around the world. The idea of calling with the tips of your fingers was a revolutionary idea that continues to set the standard for telecommunications. With advancements to cell phones, one question that is always present is pricing There may be certain factors that affect cell phone pricing such as storage, camera capabilities, and battery power. The goal of this project is to assess that, seeing if certain features of phones affect pricing in a significant way. It is an interesting and important question to answer because it can inform others about what phone features matter the most to companies that make phones as well as inform us about what features matter the most to a phone's functionality when looking to buy one. Machine learning is a reasonable approach to tackle this question because it can give us insight into why or how phones are purchased. Moreover, it can help us predict phone prices in the future based on what features they possess, which would be informed by past data on this exact matter. In other words, it would help readers assess what features are continuing to affect the price of the phone the most in the present.

# Illustration:

![Elephant](380_Final_Chart_PNG.png)

# Background and Related Works:

We looked at an article from IEEE Xplore. This article was about predicting mobile phone prices using a data set from kaggle. This article differed from ours because they were predicting phone prices with classification. They had their y variable in as a factor with 4 levels. The levels were form "low cost" to "very high cost". Some examples of their x variables were battery power and clock speed. They used several different models to predict phone price such as a decision tree and SVM. Their most accurate model was SVM with an accuracy of 94.8%.

Reference: N. Hu, "Classification of Mobile Phone Price Dataset Using Machine Learning Algorithms," 2022 3rd International Conference on Pattern Recognition and Machine Learning (PRML), Chengdu, China, 2022, pp. 438-443, doi: 10.1109/PRML56267.2022.9882236. keywords: {Support vector machines;Machine learning algorithms;Random access memory;Machine learning;Feature extraction;Mobile handsets;Batteries;computer science;machine learning;classification;price prediction},

# Data Processing:

We loaded in the data sets though the readxl package

```{r}
#| echo: false

library(glmnet)
library(readxl)
library(tidyverse)
library(corrplot)
library(torch)
library(luz)
library(dplyr)
library(broom)
library(purrr)
library(caret)
library(tibble)



df <- read_excel('smartphones_-_smartphones.xlsx')
df2 <- read_csv('Sales.csv')
```

## First Dataset

The first data set looked like this before processing.

```{r}
#| echo: false
head(df)
```

It is a tabular data set on some mobile phones. Some examples of columns in the data set are mobile which represents the name of the phone and the price of the phone.

To start off we took out the model column because it represented the names of the phones which will not impact the price. We also took out the sim column.

```{r}
#| echo: false
df <- df %>% drop_na() %>% select(!model) %>% select(!sim)
head(df)
```

### Cleaning battery column

We extracted the battery life of each phone in mAH and made the column numeric

```{r}
#| echo: false
df <- df %>% 
  mutate(battery = gsub(pattern = "mAh Battery|with|(?:[0-9]){1,3}W|Fast Charging", replacement = "", battery)) %>% mutate_at('battery', as.numeric) %>% drop_na() %>% rename('battery_mAh'='battery')
head(df)
```

### Cleaning processor variable

We extracted the power of the processor in GHz. We then made the column numeric

```{r}
#| echo: false
df$processor <- str_extract(df$processor, "\\d+\\.?\\d*\\s*GHz|\\d+\\s*GHz")

df$processor <- gsub("GHz", "", df$processor)

df <- df %>% drop_na() %>% mutate_at('processor', as.numeric) %>%rename('processor GHz)'='processor')
head(df)
```

### Cleaning os column

We noticed that because the data was unclean, some of the values that should be in the os column were in the card column. We put these value in the os column and removed the card column after. We also made the os column a factor.

```{r}
#| echo: false
for (i in 1:nrow(df)){
  if (df[i,9] == 'No FM Radio'){
    df[i,9] <- df[i,8]
  }
  else if (df[i,9] == 'Bluetooth'){
    df[i,9] <- df[i,8]
  }
}

df <- df %>% select(!card) %>% mutate_at('os', as.factor)
head(df)
```

### Cleaning camera column

We extracted the amount of mega pixels in the front camera of each phone. We made this column numeric

```{r}
#| echo: false
df$camera <- str_extract(df$camera, '[0-9]{1,2} MP Front Camera')
df$camera <- str_extract(df$camera, '[0-9]{1,2}')
df <- df %>% mutate_at('camera', as.numeric) %>% rename('f_camera_MP'='camera') %>% drop_na()
head(df)
```

### Cleaning ram column

We extracted the ram of the phones in GB and made it a factor because phones only have a few preset values for their ram

```{r}
#| echo: false
df$ram <- str_extract(df$ram, '[0-9]{1,2} GB')
df <- df %>% mutate_at('ram', as.factor) 
head(df)
```

### Cleaning Display column

We extracted the display size and the Hz of the display and turned that into two new columns. We made these new columns numeric and removed the original

```{r}
#| echo: false
df <- df %>% mutate(displaySize = as.numeric(str_extract(df$display, "\\b\\d+\\.\\d+\\b")))
```

```{r}
#| echo: false
df <- df %>% mutate(displayHz = as.numeric(str_extract(df$display, "\\b\\d+(?=\\s*Hz)")))

df <- df %>% select(!display)
head(df)
```

### Cleaning Price column

We converted the value in rupees to dollars to make it easier to understand for our audience

```{r}
#| echo: false
df <- df %>%
  mutate(price = gsub(",", "", price))
df$price <- sub("\\₹", "", df$price)
df$price <- as.numeric(df$price)

df <- df %>%
  mutate(price = round(price / 83.41, digits = 2)) %>% rename('price'='price') %>% drop_na()
head(df)
```

### General Analysis

After Cleaning:

```{r}
#| echo: false
head(df)
```

```{r}
#| echo: false
summary(df)
```

```{r}
#| echo: false
library(corrplot)

numeric_data <- df2 %>%
  select_if(is.numeric) %>% drop_na()

correlation_matrix <- cor(numeric_data)

corrplot(correlation_matrix, method = "circle")
```

## Second Dataset

The first dataset looked like this before processing

```{r}
#| echo: false
head(df2)
```

It is also a tabular data set with information on mobile phones. This data set differs from the first because it has less columns that are useful for predicting price but it has more rows.

### Cleaning P2

To start we removed unneeded columns. These were models, Camera, selling price, mobile, discount, and discount percentage. We then make all the column names lowercase. We then made all the data in the colors and brands columns lowercase. We then removed the underscore from the original_price column name. We then converted the price to dollars. We them made the memory, brands, and storage columns factors.

```{r}
#| echo: false
df2 <- df2 %>% drop_na()

df2 <- df2[,-c(2,3,6,8,10,11,12)]

names(df2) <- tolower(names(df2))

df2$brands <- tolower(df2$brands)

df2<- rename(df2, original_price = "original price")

df2 <- df2 %>% mutate(original_price = df2$original_price * 0.012)

df2$memory <- as.factor(df2$memory)

df2$storage <- as.factor(df2$storage)

df2$brands <- as.factor(df2$brands)
head(df2)
```

### General Analysis

After cleaning:

```{r}
#| echo: false
head(df2)
```

```{r}
#| echo: false
summary(df2)
```

```{r}
#| echo: false
numeric_data2 <- df %>%
  select_if(is.numeric)
correlation_matrix2 <- cor(numeric_data2)
corrplot(correlation_matrix2, method = "circle")
```

```{r}
#| echo: false
rmse <- function(y, yhat) {
sqrt(mean((y - yhat)^2))
}
```

# Architecture

The models we used to predict price were linear regression models, lasso and ridge models, stepwise regression models, and neural networks.  We fitted each of those models to both data sets.  We judged the accuracy of our models by calculating the root mean squared error for each.

Our baseline model was a simple linear regression model which we will discuss more in the next section.

Some interesting intermediate models we used were stepwise, lasso, and ridge regression models.  We used both forwards and backwards stepwise regression starting with a null model and a full model respectively. We used the glmnet library to get out lasso and ridge models.  These models introduced regularization.  

Our final model was a neural network with three hidden layers.  We used 16 nodes for the first layer, 32 for the second, and 16 for the third when we fitted the models to each dataset.  We set the the optimizer as optim_adam and had a learning rate of 0.02

# Baseline Model

To start with our introductory model, we have decided on a multi-linear regression model. This will set a foundation for the more complex models all predicting price. 

Fit Two Linear Regression Models:

```{r}
#| echo: false
lm1 <- lm(price ~ ., data = df)
lm2 <- lm(original_price ~ ., data = df2)

summary1 <- summary(lm1)
summary2 <- summary(lm2)

```

Summary of Models:

```{r}
#| echo: false
print(summary1)
print(summary2)
```

Initial Thoughts:

We would expect to see price increase as the newer specs for phones are released and are put onto the market. Thus, we would expect things like memory, storage, ram, and other specs to be significant predictors for both of our data sets. As we can see this is the true.

One noteworthy observation we noticed in both models in the high level of Adjusted R-Squared(.85 and .75). This can indicate over fitting, or it can indicator an accurate model. 


Here are some example interpretations for the first model:


Intercept: When all other predictor variables are zero, the estimated price of the product is approximately -\$5.080e+02.

Rating: For every one-unit increase in the rating, the price is estimated to increase by approximately \$1.754e+01, holding all other variables constant.

Processor(GHz): For every one-unit increase in the processor GHz, the price is estimated to increase by approximately \$1.423e+02, holding all other variables constant.



Battery(mAH): For every one-unit increase in battery mAh, the price is estimated to decrease by approximately \$4.114e-04, holding all other variables constant.

Camera(MP): For every one-unit increase in front camera megapixels, the price is estimated to decrease by approximately \$2.566, holding all other variables constant.

Operating System: Devices with Android v10.0: On average, devices with Android v10.0 have prices that are \$22.38 higher than the reference category, holding all other variables constant. 
Display size: For every one-unit increase in display size, the price is estimated to decrease by approximately \$1.389e+02, holding all other variables constant.

Display Hz: For every one-unit increase in display size, the price is estimated to decrease by approximately \$1.487, holding all other variables constant.

For the second model,

Operating System: Phones of the brands ASUS, Gionee, Google Pixel, HTC, Infinix, IQOO, Lenovo, LG, Motorola, Nokia, OPPO, POCO, Realme, Samsung, Vivo, and Xiaomi have an estimated price change of -491.83, -517.99, -2.47, -281.1, -582.7, -604.93, -504.80, -398.47, -468.75, -482.1, -552.06, -606.83, -612.25, -426.85, -566.37, and -563.31 respectively in dollars compared to the reference category holding all other variables constant.

Memory: Phones with RAM memory capacity of 1.5GB, 10MB, 100 MB, 12GB, 128MB, 153MB, 16GB, 16 MB, 2GB, 2MB, 3GB, 30MB, 32MB, 4GB, 46MB, 4G, 512MB, 6GB, 64MB, 768MB, 8GB, and 8MB have an estimated price change of -7.67,-15.23,-61.86, 469.00, -18.25, -1309.63, 731.42, -9.39, -4.51, -1.56, 64.78, 49.61, 3.63, 143.76, 5.76, -11.06, 259.28, -11.84, 203.96, 8.72, -22.95, 271.17, and 13.27 respectively in dollars compared to the reference category holding all other variables constant.


Rating: For every one-unit increase in rating, the price is estimated to increase by approximately \$119.62, holding all other variables constant.

=======

```{r}
#| echo: false
linear1rmse <- rmse(df$price, lm1$fitted.values)
print(linear1rmse)
linear2rmse <- rmse(df2$original_price, lm2$fitted.values)
print(linear2rmse)

print(mean(df$price))
print(mean(df2$original_price))

```

As you see, we have a RMSE of 98.6 and 165.5 for both data sets respectively. On average, this means our model is off by about \$98.6 and \$165.5. Since the average price of the phones is around \$345.50 and \$319.88.

The RMSE variation is likely caused by the heavy right-skewness in the price variable of more expensive phones. More expensive prices leads to an exponential decay of more expensive components in a thinner market; thus, leading to an exponentially distribution price vector column.

```{r}
#| echo: false
histogram(df$price, main = "Boxplot of Price Variable(Df1)")

histogram(df2$original_price, main = "Boxplot of Continuous Variable(DF2)")

```

Lasso and Ridge:

```{r}
#| echo: false
x1 <- data.matrix(df %>% select(!price))
y1 <- df$price
x2 <- data.matrix(df2 %>% select(!original_price))
y2 <- df2$original_price

lasso1 <- cv.glmnet(x1, y1, alpha = 1)
ridge1 <- cv.glmnet(x1, y1, alpha = 0)
lasso2 <- cv.glmnet(x2, y2, alpha = 1)
ridge2 <- cv.glmnet(x2, y2, alpha = 0)

lasso1rmse <- rmse(y1, predict(lasso1, x1))
ridge1rmse <- rmse(y1, predict(ridge1, x1))
lasso2rmse <- rmse(y2, predict(lasso2, x2))
ridge2rmse <- rmse(y2, predict(ridge2, x2))

```

```{r}
#| echo: false
par(mfrow=c(1, 2))
plot(lasso1)
plot(ridge1)
plot(lasso2)
plot(ridge2)
```

Stepwise:

```{r}
#| echo: false
null_model1 <- lm(price ~ 1, df)
forward1 <- step(null_model1, direction = 'forward', scope = formula(lm1))

null_model2 <- lm(original_price ~ 1, df2)
forward2 <- step(null_model2, direction = 'forward', scope = formula(lm2))


backward1 <- step(lm1, direction = 'backward')
backward2 <- step(lm2, direction = 'backward')


forward1rmse <- rmse(df$price, predict(forward1))
backward1rmse <- rmse(df$price, predict(backward1))
forward2rmse <- rmse(df2$original_price, predict(forward2))
backward2rmse <- rmse(df2$original_price, predict(backward2))
```



Neural Network:


```{r}
#| echo: false
normalize <- function(x) {
  (x - min(x)) / (max(x) - min(x))
}

df_normalized <- df %>%
  mutate(price = normalize(price)) 

df2_normalized <- df2 %>%
  mutate(original_price = normalize(original_price))
  
```


```{r}
#| echo: false
nn_model <- nn_module(
  initialize = function(p, q1, q2, q3){
    self$hidden1 <- nn_linear(p,q1)
    self$hidden2 <- nn_linear(q1,q2)
    self$hidden3 <- nn_linear(q2,q3)
    self$output <- nn_linear(q3,1)
    self$activation <- nn_relu()
  },
  forward = function(x){
    x %>%
      self$hidden1() %>% self$activation() %>%
      self$hidden2() %>% self$activation() %>%
      self$hidden3() %>% self$activation() %>%
      self$output()
  }
)

```

```{r}
#| echo: false
M1 <- model.matrix(price ~ 0 + . , data = df_normalized)

nn1 <- nn_model %>% 
    setup(loss = nn_mse_loss(),
        optimizer = optim_adam, 
        metrics = list(luz_metric_accuracy())) %>%
    set_hparams(p = ncol(M1), q1 = 16, q2 = 32, q3 = 16) %>%
    set_opt_hparams(lr = 0.05) %>%
    fit(data = list(
        model.matrix(price ~ 0 + ., data = df_normalized), df_normalized %>% select(price) %>% as.matrix
    ),
     epochs = 50, verbose = F)

```


```{r}
#| echo: false
nnrmse1 <- rmse(df_normalized$price, predict(nn1, model.matrix(price ~ 0 + ., data = df_normalized))) %>% as.double()
```

```{r}
#| echo: false
M2 <- model.matrix(normalize(original_price) ~ 0 + . , data = df2)

nn2 <- nn_model %>% 
    setup(loss = nn_mse_loss(),
        optimizer = optim_adam, 
        metrics = list(luz_metric_accuracy())) %>%
    set_hparams(p = ncol(M2), q1 = 16, q2 = 32, q3 = 16) %>%
    set_opt_hparams(lr = 0.05) %>%
    fit(data = list(
        model.matrix(original_price ~ 0 + ., data = df2_normalized), df2_normalized %>% select(original_price) %>% as.matrix
    ),
     epochs = 50, verbose = F)
```

```{r}
#| echo: false


nnrmse2 <- rmse(df2_normalized$original_price, predict(nn2, model.matrix(original_price ~ 0 + ., data = df2_normalized))) %>% as.double()


```

Interpreting Normalized RMSE: 

```{r}
#| echo: false
price_diff1<- nnrmse1 * mean(df$price)
price_diff2<- nnrmse2 * mean(df2$original_price)
print(price_diff1)
print(price_diff2)
```


Normalizing our price column leads to a different interpretation of our neural network's RMSE. On average, our first data set's model is off by about 10%. This means price can be off between +- $34.66. The best performing model by far. For our second data set, the normalized rmse is .221. This means on average, the second data set's model is off by 22.1%. Thus, price can be off by between +- $68.01. Another good model in relation to the others!


# Quantitative results

As stated before, we judged the models by assessing the root mean squared error for each.  We chose this method because we are working with regression models.  We did not use any form of cross validation because we were running into errors.  Because both data sets include factors columns with a lot of levels, the testing data was giving the models levels that were not included in the training data.   


```{r}
#| echo: false
summary_table <- data.frame(Model = c('Linear', 'Lasso', 'Ridge', 'Forwards', 'Backwards', 'NNetwork(normalized)', 'Linear', 'Lasso', 'Ridge', 'Forwards', 'Backwards', 'NNetwork(normalized)'), Dataset = c(1,1,1,1,1,1,2,2,2,2,2,2), RMSE = c(linear1rmse, lasso1rmse, ridge1rmse, forward1rmse, backward1rmse, nnrmse1, linear2rmse, lasso2rmse, ridge2rmse, forward2rmse, backward2rmse, nnrmse2))

summary_table
```

# Qualitative results

```{r}
#| echo: false
df_test <- df


iphone13 <- c(599, 86, 2.65, '4 GB', 3227, 12, 'iOS v15.0', 6.1, 120)
motogstylus <- c(176, 86, 1.8, '4 GB', 5000, 16, 'Android v11', 6.8, 120)
pixel <- c(199, 88, 1.6, '4 GB', 2770, 12.3, 'Android v10', 5.0, 60)
df_ex1 <- rbind(df_test, iphone13, motogstylus, pixel)

df_ex1$price <- as.numeric(df_ex1$price)
df_ex1$battery_mAh <- as.numeric(df_ex1$battery_mAh)
df_ex1$rating <- as.numeric(df_ex1$rating)
df_ex1$`processor GHz)` <- as.numeric(df_ex1$`processor GHz)`)
df_ex1$f_camera_MP <- as.numeric(df_ex1$f_camera_MP)
df_ex1$displaySize <- as.numeric(df_ex1$displaySize)
df_ex1$displayHz <- as.numeric(df_ex1$displayHz)

df_ex1_n <- df_ex1  %>% mutate(price = normalize(price))

p_ex1 <- predict(nn1, newdata = model.matrix(price ~ 0 + ., data = df_ex1_n)) %>% as.double

```


These are the normalized price predictions for the phone 13, motog stylus, and the google pixel.

```{r}
#| echo: false
p_ex1[507:509]
```

```{r}
#| echo: false
iphone15 <- c('apple', '6 GB', '128 GB', 4.6, 799)
samsung_s24 <- c('samsung', '12 GB', '256 GB', 4.5, 1400)
vivoy18 <- c('vivo', '8 GB', '256 GB', 4.0, 107.87)

df_ex2 <- rbind(df2, iphone15, samsung_s24, vivoy18)
df_ex2$rating <- as.numeric(df_ex2$rating)
df_ex2$original_price <- as.numeric(df_ex2$original_price)

df_ex2_n <- df_ex2  %>% mutate(original_price = normalize(original_price))

p_ex2 <- predict(nn2, newdata = model.matrix(original_price ~ 0 + ., data = df_ex2_n)) %>% as.double

```


These are the normalized price predictions for the iphone 15, samsung s24 ultra, and vivo y18.

```{r}
#| echo: false
p_ex2[2898:2900]
```


# Discussion

The journey of our predictive model development came with many unexpected challenges and barriers imposed by the data itself, but we as a team were able to overcome these and create a clear final model to reasonably predict phone prices. 

In terms of the model's performance our baseline model performed the best (disregarding the final model). Feature selection approaches like lasso, ridge, and step wise regression were not effective in lowering the average error. This is likely caused by the large amount of factors in our data set which posed a huge problem in training the data. For example, cross validation was not possible to test the data because of the large amount of column factors present in the test data but not the training. 

For our final model's performance, we are pleasantly surprised with the end result. Initially, a basic neural network was not effective in predicting phone prices, but normalizing the data to improve gradient descent made our final models within 10 and 15 percent accuracy. A huge improvement regarding the other models. We think this was likely caused by the factors making the gradient descent computations saturated and inaccurate. Thus, normalizing the data improved this error margin in the training process. 

Some interesting results we noticed were the lack of improvement in the feature selection models. We were not expecting feature selection to be so ineffective. On the contrary, we thought the high-dimensions of the data set caused by the factors would lead to feature selection being the most effective; however, this proved to be the opposite. High-dimensional factors caused feature selection to be completely ineffective.
I didn't proof read this at all but lmk what you guys think on snap or in here




# Ethical Considerations 

There are clear limitations to the model.  For one, the first dataset used in the creation of these models is smaller with about 500 rows, which can limit how effective models can be in terms of efficiency and accuracy.  For the second dataset, there are only four predictor variables for the price after cleaning the dataset, which can also limit the extent of how efficient a model can be because it is not considering all aspects that may contribute to the price of a phone.  As stated earlier, cross validation was not possible to perform on this group of data, so we were not able to fully test the accuracy of the models as well as detect any possible overfitting.



# Appendix

Loading in data:
```{r}
library(glmnet)
library(readxl)
library(tidyverse)
library(corrplot)
library(torch)
library(luz)
library(dplyr)
library(broom)
library(purrr)
library(caret)
library(tibble)



df <- read_excel('smartphones_-_smartphones.xlsx')
df2 <- read_csv('Sales.csv')
```


Cleaning First Data set:
```{r}
df <- df %>% drop_na() %>% select(!model) %>% select(!sim)

df <- df %>% 
  mutate(battery = gsub(pattern = "mAh Battery|with|(?:[0-9]){1,3}W|Fast Charging", replacement = "", battery)) %>% mutate_at('battery', as.numeric) %>% drop_na() %>% rename('battery_mAh'='battery')

df$processor <- str_extract(df$processor, "\\d+\\.?\\d*\\s*GHz|\\d+\\s*GHz")
df$processor <- gsub("GHz", "", df$processor)
df <- df %>% drop_na() %>% mutate_at('processor', as.numeric) %>%rename('processor GHz)'='processor')
head(df)

for (i in 1:nrow(df)){
  if (df[i,9] == 'No FM Radio'){
    df[i,9] <- df[i,8]
  }
  else if (df[i,9] == 'Bluetooth'){
    df[i,9] <- df[i,8]
  }
}
df <- df %>% select(!card) %>% mutate_at('os', as.factor)

df$camera <- str_extract(df$camera, '[0-9]{1,2} MP Front Camera')
df$camera <- str_extract(df$camera, '[0-9]{1,2}')
df <- df %>% mutate_at('camera', as.numeric) %>% rename('f_camera_MP'='camera') %>% drop_na()

df$ram <- str_extract(df$ram, '[0-9]{1,2} GB')
df <- df %>% mutate_at('ram', as.factor) 

df <- df %>% mutate(displaySize = as.numeric(str_extract(df$display, "\\b\\d+\\.\\d+\\b")))
df <- df %>% mutate(displayHz = as.numeric(str_extract(df$display, "\\b\\d+(?=\\s*Hz)")))
df <- df %>% select(!display)

df <- df %>%
  mutate(price = gsub(",", "", price))
df$price <- sub("\\₹", "", df$price)
df$price <- as.numeric(df$price)
df <- df %>%
  mutate(price = round(price / 83.41, digits = 2)) %>% rename('price'='price') %>% drop_na()
```


Cleaning second Data set:
```{r}
df2 <- df2 %>% drop_na()

df2 <- df2[,-c(2,3,6,8,10,11,12)]

names(df2) <- tolower(names(df2))

df2$brands <- tolower(df2$brands)

df2<- rename(df2, original_price = "original price")

df2 <- df2 %>% mutate(original_price = df2$original_price * 0.012)

df2$memory <- as.factor(df2$memory)

df2$storage <- as.factor(df2$storage)

df2$brands <- as.factor(df2$brands)
```


First two linear regression models
```{r}
lm1 <- lm(price ~ ., data = df)
lm2 <- lm(original_price ~ ., data = df2)

summary1 <- summary(lm1)
summary2 <- summary(lm2)

print(summary1)
print(summary2)
```

```{r}
linear1rmse <- rmse(df$price, lm1$fitted.values)
print(linear1rmse)
linear2rmse <- rmse(df2$original_price, lm2$fitted.values)
print(linear2rmse)

print(mean(df$price))
print(mean(df2$original_price))
```


Histograms:
```{r}
histogram(df$price, main = "Boxplot of Price Variable(Df1)")

histogram(df2$original_price, main = "Boxplot of Continuous Variable(DF2)")
```


Lasso and Ridge:
```{r}
x1 <- data.matrix(df %>% select(!price))
y1 <- df$price
x2 <- data.matrix(df2 %>% select(!original_price))
y2 <- df2$original_price

lasso1 <- cv.glmnet(x1, y1, alpha = 1)
ridge1 <- cv.glmnet(x1, y1, alpha = 0)
lasso2 <- cv.glmnet(x2, y2, alpha = 1)
ridge2 <- cv.glmnet(x2, y2, alpha = 0)

lasso1rmse <- rmse(y1, predict(lasso1, x1))
ridge1rmse <- rmse(y1, predict(ridge1, x1))
lasso2rmse <- rmse(y2, predict(lasso2, x2))
ridge2rmse <- rmse(y2, predict(ridge2, x2))
```


Stepwise:
```{r}
null_model1 <- lm(price ~ 1, df)
forward1 <- step(null_model1, direction = 'forward', scope = formula(lm1))

null_model2 <- lm(original_price ~ 1, df2)
forward2 <- step(null_model2, direction = 'forward', scope = formula(lm2))


backward1 <- step(lm1, direction = 'backward')
backward2 <- step(lm2, direction = 'backward')


forward1rmse <- rmse(df$price, predict(forward1))
backward1rmse <- rmse(df$price, predict(backward1))
forward2rmse <- rmse(df2$original_price, predict(forward2))
backward2rmse <- rmse(df2$original_price, predict(backward2))
```


Normalization Function:
```{r}
normalize <- function(x) {
  (x - min(x)) / (max(x) - min(x))
}

df_normalized <- df %>%
  mutate(price = normalize(price)) 

df2_normalized <- df2 %>%
  mutate(original_price = normalize(original_price))
  
```


Setting up NNetwork:
```{r}
nn_model <- nn_module(
  initialize = function(p, q1, q2, q3){
    self$hidden1 <- nn_linear(p,q1)
    self$hidden2 <- nn_linear(q1,q2)
    self$hidden3 <- nn_linear(q2,q3)
    self$output <- nn_linear(q3,1)
    self$activation <- nn_relu()
  },
  forward = function(x){
    x %>%
      self$hidden1() %>% self$activation() %>%
      self$hidden2() %>% self$activation() %>%
      self$hidden3() %>% self$activation() %>%
      self$output()
  }
)

```


Fitting Neural Network
```{r}
M1 <- model.matrix(price ~ 0 + . , data = df_normalized)

nn1 <- nn_model %>% 
    setup(loss = nn_mse_loss(),
        optimizer = optim_adam, 
        metrics = list(luz_metric_accuracy())) %>%
    set_hparams(p = ncol(M1), q1 = 16, q2 = 32, q3 = 16) %>%
    set_opt_hparams(lr = 0.05) %>%
    fit(data = list(
        model.matrix(price ~ 0 + ., data = df_normalized), df_normalized %>% select(price) %>% as.matrix
    ),
     epochs = 50, verbose = F)

```

```{r}
nnrmse1 <- rmse(df_normalized$price, predict(nn1, model.matrix(price ~ 0 + ., data = df_normalized))) %>% as.double()
```

```{r}
M2 <- model.matrix(normalize(original_price) ~ 0 + . , data = df2)

nn2 <- nn_model %>% 
    setup(loss = nn_mse_loss(),
        optimizer = optim_adam, 
        metrics = list(luz_metric_accuracy())) %>%
    set_hparams(p = ncol(M2), q1 = 16, q2 = 32, q3 = 16) %>%
    set_opt_hparams(lr = 0.05) %>%
    fit(data = list(
        model.matrix(original_price ~ 0 + ., data = df2_normalized), df2_normalized %>% select(original_price) %>% as.matrix
    ),
     epochs = 50, verbose = F)
```

```{r}
nnrmse2 <- rmse(df2_normalized$original_price, predict(nn2, model.matrix(original_price ~ 0 + ., data = df2_normalized))) %>% as.double()
```


Interpreting Normalized RMSE: 
```{r}
price_diff1<- nnrmse1 * mean(df$price)
price_diff2<- nnrmse2 * mean(df2$original_price)
print(price_diff1)
print(price_diff2)
```


Summary Table:
```{r}
summary_table <- data.frame(Model = c('Linear', 'Lasso', 'Ridge', 'Forwards', 'Backwards', 'NNetwork(normalized)', 'Linear', 'Lasso', 'Ridge', 'Forwards', 'Backwards', 'NNetwork(normalized)'), Dataset = c(1,1,1,1,1,1,2,2,2,2,2,2), RMSE = c(linear1rmse, lasso1rmse, ridge1rmse, forward1rmse, backward1rmse, nnrmse1, linear2rmse, lasso2rmse, ridge2rmse, forward2rmse, backward2rmse, nnrmse2))

summary_table
```


Qualitative results:
```{r}
df_test <- df


iphone13 <- c(599, 86, 2.65, '4 GB', 3227, 12, 'iOS v15.0', 6.1, 120)
motogstylus <- c(176, 86, 1.8, '4 GB', 5000, 16, 'Android v11', 6.8, 120)
pixel <- c(199, 88, 1.6, '4 GB', 2770, 12.3, 'Android v10', 5.0, 60)
df_ex1 <- rbind(df_test, iphone13, motogstylus, pixel)

df_ex1$price <- as.numeric(df_ex1$price)
df_ex1$battery_mAh <- as.numeric(df_ex1$battery_mAh)
df_ex1$rating <- as.numeric(df_ex1$rating)
df_ex1$`processor GHz)` <- as.numeric(df_ex1$`processor GHz)`)
df_ex1$f_camera_MP <- as.numeric(df_ex1$f_camera_MP)
df_ex1$displaySize <- as.numeric(df_ex1$displaySize)
df_ex1$displayHz <- as.numeric(df_ex1$displayHz)

df_ex1_n <- df_ex1  %>% mutate(price = normalize(price))

p_ex1 <- predict(nn1, newdata = model.matrix(price ~ 0 + ., data = df_ex1_n)) %>% as.double
```

```{r}
p_ex1[507:509]
```

```{r}
iphone15 <- c('apple', '6 GB', '128 GB', 4.6, 799)
samsung_s24 <- c('samsung', '12 GB', '256 GB', 4.5, 1400)
vivoy18 <- c('vivo', '8 GB', '256 GB', 4.0, 107.87)

df_ex2 <- rbind(df2, iphone15, samsung_s24, vivoy18)
df_ex2$rating <- as.numeric(df_ex2$rating)
df_ex2$original_price <- as.numeric(df_ex2$original_price)

df_ex2_n <- df_ex2  %>% mutate(original_price = normalize(original_price))

p_ex2 <- predict(nn2, newdata = model.matrix(original_price ~ 0 + ., data = df_ex2_n)) %>% as.double

```

```{r}
p_ex2[2898:2900]
```







